{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Activation Function\n",
    "\n",
    "This example shows how to use a custom activation function in binet. We will implement this function both on the CPU and on the GPU. If you just want to quickly try something out, you can do without the GPU part.\n",
    "\n",
    "Our activation function will be $h = max(0, \\log(x+1))$. This might not make much sense as an activation function, but serves as a quick example as it's easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from binet import *\n",
    "from binet import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "__cuda_actfun =  ElementwiseKernel('float* x, float* o', \n",
    "                                   'o[i] = x[i] > 0 ? logf(x[i]+1.0f) : 0', 'actfun_eltw')\n",
    "__cuda_actfun_delta = ElementwiseKernel('float* d, float* a, float* x', \n",
    "                                        'd[i] *= x[i] > 0 ? 1.0f / (x[i]+1.0f) : 0;', 'dactfun_eltw')\n",
    "\n",
    "def my_activation(x, out=None, stream=None):\n",
    "    if out is None:\n",
    "        out = op.empty_like(x)\n",
    "    if isinstance(x, op.gpuarray.GPUArray):\n",
    "        __cuda_actfun(x, out, stream=stream)\n",
    "    else:\n",
    "        out[:] = np.where(x > 0, np.log(x+1), 0)\n",
    "    return out  \n",
    "    \n",
    "\n",
    "def my_activation_delta(D, A, X, stream=None):\n",
    "    \"\"\" Calculates D *= (a > 0)\"\"\"\n",
    "    if isinstance(D, op.gpuarray.GPUArray):\n",
    "        __cuda_actfun_delta(D, A, X, stream=stream)\n",
    "    else:\n",
    "        D *= np.where(X > 0, 1.0 / (X+1), 0)\n",
    "    return D\n",
    "\n",
    "\n",
    "# Register the new activation function\n",
    "from binet import layers\n",
    "layers.BasicLayer.activationfunctions['myfunc'] = (my_activation, my_activation_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:binet.util:CUDA not initialized, initializing GPU 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25:\tTrain-Error: 0.007736\tVal-Error: 0.934976\tVal-Score: 71.4500%\t(16.22s)\n",
      " 50:\tTrain-Error: 0.003295\tVal-Error: 1.165723\tVal-Score: 71.4000%\t(31.05s)\n",
      " 75:\tTrain-Error: 0.001304\tVal-Error: 1.051073\tVal-Score: 77.2500%\t(45.92s)\n",
      "100:\tTrain-Error: 0.000234\tVal-Error: 1.118514\tVal-Score: 78.8000%\t(60.72s)\n"
     ]
    }
   ],
   "source": [
    "from binet.util import train\n",
    "import time\n",
    "\n",
    "x_tr, y_tr, x_va, y_va, x_te, y_te = load_dataset('mnist_bgimg', return_testset=True)\n",
    "    \n",
    "params = {'layersizes': (x_tr.shape[1], 1024, 1024, y_tr.shape[1]), 'max_iter':125, \n",
    "          'learning_rate':0.05, 'momentum': 0.8, 'verbose': True,\n",
    "          'activation':'myfunc'}\n",
    "\n",
    "net = NeuralNet( **params)\n",
    "net = train(net, (x_tr, y_tr, x_va, y_va), skip_output=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.score(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host:                hawk: Linux-3.10.0-229.14.1.el7.x86_64-x86_64-with-centos-7.1.1503-Core\n",
      "Date:                2015-10-12 10:44:29.097192\n",
      "Python version:      3.4.3 (default, Mar  1 2015, 13:48:33) \n",
      "                     [GCC 4.8.2 20140120 (Red Hat 4.8.2-16)]\n",
      "repository version:  commit abf08963b888becba55836a9a4e413f715dced4d\n",
      "\n",
      "loaded modules:\n",
      "\t IPython 4.0.0\n",
      "\t binet 2015.10\n",
      "\t matplotlib 1.4.3\n",
      "\t numpy 1.9.2\n",
      "\t pandas 0.16.0\n",
      "\t scipy 0.16.0\n",
      "\t sklearn 0.16.1\n"
     ]
    }
   ],
   "source": [
    "print_system_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
